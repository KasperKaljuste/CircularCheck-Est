{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get URLs from Sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get URLs from sitemap\n",
    "\n",
    "sitemap_url = 'https://www.postimees.ee/sitemap/news'\n",
    "\n",
    "response = requests.get(sitemap_url)\n",
    "soup = BeautifulSoup(response.content, 'xml')\n",
    "\n",
    "url_tags = soup.find_all(\"url\")\n",
    "\n",
    "urls = [url.loc.text for url in url_tags]\n",
    "\n",
    "df = pd.DataFrame(urls, columns=[\"URL\"])\n",
    "#df.to_excel(\"Postimees_URLs.xlsx\", index = False)\n",
    "df.head()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect URLs from Specified Subsections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Timeout duration in seconds\n",
    "TIMEOUT = 30\n",
    "\n",
    "# Base API URL for article fetching\n",
    "API_URL = \"https://services.postimees.ee/rest/v1/sections/{section}/articles\"\n",
    "\n",
    "# Function to fetch articles and their metadata from the API\n",
    "def fetch_articles(section_id, limit, offset):\n",
    "    params = {\n",
    "        \"limit\": limit,\n",
    "        \"offset\": offset,\n",
    "        \"filterSectionFeedArticles\": \"true\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(API_URL.format(section=section_id), params=params, timeout=TIMEOUT)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            articles = []\n",
    "            for article in data:\n",
    "                article_data = {\n",
    "                    \"id\": article[\"id\"],\n",
    "                    \"url\": f\"https://www.postimees.ee/{article['id']}\",\n",
    "                    \"slug\": article.get(\"slug\"),\n",
    "                    \"headline\": article.get(\"headline\"),\n",
    "                    \"dateCreated\": article.get(\"dateCreated\"),\n",
    "                    \"dateModified\": article.get(\"dateModified\"),\n",
    "                    \"datePublished\": article.get(\"datePublished\"),\n",
    "                    \"isPremium\": article.get(\"isPremium\", False),\n",
    "                    \"relatedArticleCount\": article.get(\"meta\", {}).get(\"relatedArticleCount\"),\n",
    "                    \"wordCount\": article.get(\"meta\", {}).get(\"wordCount\"),\n",
    "                    \"authors\": \", \".join(\n",
    "                        f\"{author.get('name')} ({author.get('authorType')})\"\n",
    "                        for author in article.get(\"authors\", [])\n",
    "                    ),\n",
    "                    \"terms\": \", \".join(term.get(\"term\") for term in article.get(\"terms\", [])),\n",
    "                    \"sections\": \", \".join(section.get(\"name\") for section in article.get(\"sections\", []))\n",
    "                }\n",
    "                articles.append(article_data)\n",
    "            return articles\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for section {section_id} at offset {offset}: {e}\")\n",
    "        return []\n",
    "    return []\n",
    "\n",
    "# Main function to collect articles\n",
    "def collect_articles(section_name, section_id, max_articles):\n",
    "    collected_articles = []\n",
    "    limit = 100\n",
    "    offset = 0\n",
    "\n",
    "    with tqdm(total=max_articles, desc=f\"Scraping {section_name}\") as pbar:\n",
    "        while len(collected_articles) < max_articles and (offset + limit) <= 10000:\n",
    "            articles = fetch_articles(section_id, limit, offset)\n",
    "            if not articles:\n",
    "                break\n",
    "            collected_articles.extend(articles)\n",
    "            offset += limit\n",
    "            pbar.update(len(articles))\n",
    "\n",
    "    return collected_articles[:max_articles]\n",
    "\n",
    "# Save articles to CSV\n",
    "def save_to_csv(section_name, articles):\n",
    "    os.makedirs(\"postimees_sections\", exist_ok=True)\n",
    "    file_path = os.path.join(\"postimees_sections\", f\"{section_name}.csv\")\n",
    "    pd.DataFrame(articles).to_csv(file_path, index=False)\n",
    "    print(f\"Saved {len(articles)} articles to {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define sections to scrape (name and section ID)\n",
    "    sections = {\n",
    "        #\"eesti\": 122,\n",
    "        \"majandus\": 517,\n",
    "        \"maailm\": 123,\n",
    "        \"sport\": 124,\n",
    "        \"arvamus\": 127,\n",
    "        \"kultuur\": 2187,\n",
    "        \"haridus\": 5304,\n",
    "        \"teadus\": 3371,\n",
    "        \"soda\": 5821,\n",
    "        \"koroona\": 5246\n",
    "    }\n",
    "\n",
    "    # Set the number of articles to scrape per section\n",
    "    max_articles = 10_000  # 10,000 is the maximum the API allows\n",
    "\n",
    "    # Iterate over sections and scrape articles\n",
    "    for section_name, section_id in sections.items():\n",
    "        print(f\"Starting scrape for section: {section_name}\")\n",
    "        articles = collect_articles(section_name, section_id, max_articles)\n",
    "        if articles:\n",
    "            save_to_csv(section_name, articles)\n",
    "        else:\n",
    "            print(f\"No articles found for section: {section_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not selenium (Fetch and Update Articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Load cookies from a JSON file\n",
    "def load_cookies(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        cookies = json.load(file)\n",
    "    # Convert cookies to a format compatible with requests\n",
    "    return {cookie[\"name\"]: cookie[\"value\"] for cookie in cookies}\n",
    "\n",
    "# Fetch article content using requests\n",
    "def fetch_article_content(session, article_url, article_id):\n",
    "    try:\n",
    "        response = session.get(article_url, timeout=10)  # Adjust timeout as needed\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Locate the correct article by matching data-article-id\n",
    "            article_selector = f'article[data-article-id=\"{article_id}\"]'\n",
    "            article_element = soup.select_one(article_selector)\n",
    "\n",
    "            if not article_element:\n",
    "                print(f\"Article with id {article_id} not found in {article_url}\")\n",
    "                return None, None\n",
    "\n",
    "            # Locate elements with the specific class\n",
    "            content_items = article_element.find_all(\n",
    "                lambda tag: tag.name == \"div\" and\n",
    "                            \"article-body__item--htmlElement\" in tag.get(\"class\", [])\n",
    "            )\n",
    "\n",
    "            if not content_items:\n",
    "                print(f\"No content containers found for {article_id} in {article_url}\")\n",
    "                return None, None\n",
    "\n",
    "            # Extract text from <p> and <h2> and hrefs from <a> tags within each container\n",
    "            content = []\n",
    "            hrefs = []\n",
    "            for container in content_items:\n",
    "                paragraphs_and_headings = container.find_all([\"p\", \"h2\"])\n",
    "                content.extend(element.get_text(strip=True) for element in paragraphs_and_headings)\n",
    "                links = container.find_all(\"a\", href=True)\n",
    "                hrefs.extend(link[\"href\"] for link in links)\n",
    "\n",
    "                content_text = \" \".join(content).strip() if content else None\n",
    "                hrefs_list = \", \".join(hrefs) if hrefs else None\n",
    "            return content_text, hrefs_list\n",
    "        else:\n",
    "            print(f\"Failed to fetch {article_url}. Status code: {response.status_code}\")\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {article_url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def update_csv_with_content(session, file_path, delay):\n",
    "    if not file_path:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    # Read the existing CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    if \"content\" not in df.columns:\n",
    "        df[\"content\"] = None\n",
    "    if \"hrefs\" not in df.columns:\n",
    "        df[\"hrefs\"] = None\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Updating {file_path}\"):\n",
    "        if not pd.isnull(row[\"content\"]) and not pd.isnull(row[\"hrefs\"]):\n",
    "            continue\n",
    "        article_url = row[\"url\"]\n",
    "        article_id = row['id']\n",
    "        content, hrefs = fetch_article_content(session, article_url, article_id)\n",
    "        if content:\n",
    "            df.at[index, \"content\"] = content\n",
    "        if hrefs:\n",
    "            df.at[index, \"hrefs\"] = hrefs\n",
    "        time.sleep(delay)\n",
    "\n",
    "    # Save the updated CSV\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated file saved to {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load cookies\n",
    "    cookies_file = \"www.postimees.ee_cookies.json\"  # Update with actual path\n",
    "    cookies = load_cookies(cookies_file)\n",
    "\n",
    "    # Set up a requests session\n",
    "    session = requests.Session()\n",
    "    session.cookies.update(cookies)\n",
    "\n",
    "    # Define the sections to update\n",
    "    sections = [\"koroona\"] #, \"sport\", \"arvamus\", \"kultuur\", \"haridus\", \"teadus\", \"soda\", \"koroona\"\n",
    "    base_path = \"postimees_sections\"\n",
    "\n",
    "    for section_name in sections:\n",
    "        csv_file = f\"{base_path}/{section_name}.csv\"\n",
    "        print(f\"Processing {csv_file}...\")\n",
    "        update_csv_with_content(session, csv_file, delay=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def check_and_refresh_cookies(cookie_file):\n",
    "    with open(cookie_file, \"r\") as file:\n",
    "        cookies = json.load(file)\n",
    "\n",
    "    for cookie in cookies:\n",
    "        if \"expirationDate\" in cookie:\n",
    "            if time.time() > cookie[\"expirationDate\"]:\n",
    "                print(f\"Cookie {cookie['name']} has expired.\")\n",
    "                #TODO Hande relogin or cookie update\n",
    "\n",
    "    return cookies\n",
    "\n",
    "#check_and_refresh_cookies(\"www.postimees.ee_cookies.json\")\n",
    "\n",
    "def load_cookies(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "def add_cookies_to_driver(driver, cookies, base_url):\n",
    "    driver.get(base_url)  # Open the base URL\n",
    "    time.sleep(2)  # Ensure the page is fully loaded\n",
    "    for cookie in cookies:\n",
    "        driver.add_cookie({\n",
    "            \"name\": cookie[\"name\"],\n",
    "            \"value\": cookie[\"value\"],\n",
    "            \"domain\": cookie[\"domain\"],\n",
    "            \"path\": cookie[\"path\"],\n",
    "            \"secure\": cookie.get(\"secure\", False),\n",
    "            \"httpOnly\": cookie.get(\"httpOnly\", False),\n",
    "        })\n",
    "\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    #chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    driver = webdriver.Chrome(service=Service(\"chromedriver-win64/chromedriver.exe\"), options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def fetch_article_content(driver, article_url, article_id):\n",
    "    try:\n",
    "        driver.get(article_url)\n",
    "        time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "        # Locate the correct article by matching data-article-id\n",
    "        article_selector = f'article[data-article-id=\"{article_id}\"]'\n",
    "        article_element = driver.find_element(By.CSS_SELECTOR, article_selector)\n",
    "\n",
    "        if not article_element:\n",
    "            print(f\"Article with id {article_id} not found on {article_url}\")\n",
    "            return None\n",
    "\n",
    "        # Locate all content containers with relevant classes\n",
    "        content_containers = article_element.find_elements(By.XPATH, \".//div[contains(@class, 'article-body-content')]\")\n",
    "\n",
    "        if not content_containers:\n",
    "            print(f\"No content containers found for article {article_id} on {article_url}\")\n",
    "            return None\n",
    "\n",
    "        # Extract <p> and <h2> elements within each container\n",
    "        content = []\n",
    "        for container in content_containers:\n",
    "            paragraphs_and_headings = container.find_elements(By.XPATH, \".//p | .//h2\")\n",
    "            content.extend(element.text.strip() for element in paragraphs_and_headings if element.text.strip())\n",
    "\n",
    "        return \" \".join(content).strip() if content else None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch content for {article_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def update_csv_with_content(driver, file_path):\n",
    "    if not file_path:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    # Read the existing CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    if \"content\" not in df.columns:\n",
    "        df[\"content\"] = None\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Updating {file_path}\"):\n",
    "        if pd.isnull(row[\"content\"]):  # Skip articles that already have content\n",
    "            article_url = row[\"url\"]\n",
    "            article_id = str(row[\"id\"])\n",
    "            content = fetch_article_content(driver, article_url, article_id)\n",
    "            if content:\n",
    "                df.at[index, \"content\"] = content\n",
    "\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated file saved to {file_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load cookies\n",
    "    cookies_file = \"www.postimees.ee_cookies.json\"\n",
    "    cookies = load_cookies(cookies_file)\n",
    "    \n",
    "    # Set up the Selenium driver\n",
    "    driver = setup_driver()\n",
    "    \n",
    "    try:\n",
    "        # Add cookies to the driver\n",
    "        base_url = \"https://www.postimees.ee/\"\n",
    "        add_cookies_to_driver(driver, cookies, base_url)\n",
    "        \n",
    "        # Define the sections to update\n",
    "        sections = [\"koroona\"]\n",
    "        base_path = \"postimees_sections\"\n",
    "\n",
    "        for section_name in sections:\n",
    "            csv_file = f\"{base_path}/{section_name}.csv\"\n",
    "            print(f\"Processing {csv_file}...\")\n",
    "            update_csv_with_content(driver, csv_file)\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cookie check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "def check_and_refresh_cookies(cookie_file):\n",
    "    with open(cookie_file, \"r\") as file:\n",
    "        cookies = json.load(file)\n",
    "\n",
    "    for cookie in cookies:\n",
    "        if \"expirationDate\" in cookie:\n",
    "            if time.time() > cookie[\"expirationDate\"]:\n",
    "                print(f\"Cookie {cookie['name']} has expired.\")\n",
    "                #TODO Hande relogin or cookie update\n",
    "\n",
    "    return cookies\n",
    "\n",
    "check_and_refresh_cookies(\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "circularcheck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
