{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get URLs from Sitemap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Get URLs from sitemap\n",
        "\n",
        "sitemap_url = 'https://www.delfi.ee/sitemap/news-1.xml'\n",
        "\n",
        "response = requests.get(sitemap_url)\n",
        "soup = BeautifulSoup(response.content, 'xml')\n",
        "\n",
        "url_tags = soup.find_all(\"url\")\n",
        "\n",
        "urls = [url.loc.text for url in url_tags]\n",
        "\n",
        "df = pd.DataFrame(urls, columns=[\"URL\"])\n",
        "df.to_excel(\"Delfi_URLs.xlsx\", index = False)\n",
        "df.head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get URLs from sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "BnXX_R4VvuFp",
        "outputId": "cccd80d8-86c7-438d-e0b7-1da1b5184d66"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# Base API URL for Delfi\n",
        "API_URL = \"https://content.api.delfi.ee/content/v3/graphql\"\n",
        "\n",
        "# Query parameters for fetching articles\n",
        "QUERY_PARAMS = {\n",
        "    \"operationName\": \"portal_root_getCategories\",\n",
        "    \"extensions\": {\n",
        "        \"persistedQuery\": {\n",
        "            \"version\": 1,\n",
        "            \"sha256Hash\": \"7bf55f21032140dc9aab84161289df72486f220c5a7d46084c77405c1cda83ed\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to fetch only the `id` and `url`\n",
        "def fetch_articles(section_id, limit, offset):\n",
        "    variables = {\n",
        "        \"getCount\": True,\n",
        "        \"issueOnly\": \"false\",\n",
        "        \"offset\": offset,\n",
        "        \"limit\": limit,\n",
        "        \"id\": section_id,\n",
        "        \"domain\": \"www.delfi.ee\",\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            API_URL,\n",
        "            params={\n",
        "                \"operationName\": QUERY_PARAMS[\"operationName\"],\n",
        "                \"variables\": json.dumps(variables),\n",
        "                \"extensions\": json.dumps(QUERY_PARAMS[\"extensions\"]),\n",
        "            },\n",
        "            timeout=30,\n",
        "        )\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            # Get articles or return an empty list if structure is invalid\n",
        "            items = data.get(\"data\", {}).get(\"headlines\", {}).get(\"items\", [])\n",
        "            return [{\"id\": article[\"id\"], \"url\": f\"https://www.delfi.ee/{article['id']}\"} for article in items]\n",
        "        else:\n",
        "            print(f\"Request failed with status code {response.status_code}\")\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching articles: {e}\")\n",
        "        return []\n",
        "\n",
        "# Main function to collect articles\n",
        "def collect_articles(section_name, section_id):\n",
        "    collected_articles = []\n",
        "    limit = 41  # Delfi API limit\n",
        "    offset = 0\n",
        "\n",
        "    with tqdm(total=max_articles, desc=f\"Scraping {section_name}\") as pbar:\n",
        "            while len(collected_articles) < max_articles:\n",
        "                # Stop if we're about to exceed the API limit of 10,000\n",
        "                if offset >= 10000:\n",
        "                    break\n",
        "                \n",
        "                # Adjust the limit for the last batch if needed\n",
        "                batch_limit = min(limit, 10000 - offset)\n",
        "\n",
        "                articles = fetch_articles(section_id, batch_limit, offset)\n",
        "                if not articles:\n",
        "                    break\n",
        "\n",
        "                collected_articles.extend(articles)\n",
        "                offset += batch_limit\n",
        "                pbar.update(len(articles))\n",
        "\n",
        "    return collected_articles\n",
        "\n",
        "# Save articles to CSV\n",
        "def save_to_csv(section_name, articles):\n",
        "    os.makedirs(\"delfi_sections\", exist_ok=True)\n",
        "    file_path = os.path.join(\"delfi_sections\", f\"{section_name}.csv\")\n",
        "    pd.DataFrame(articles).to_csv(file_path, index=False)\n",
        "    print(f\"Saved {len(articles)} articles to {file_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define sections to scrape (name and section ID)\n",
        "    sections = {\n",
        "        #\"eesti\": 120,\n",
        "        #\"maailm\": 123,\n",
        "        #\"jalgpall\": 24314751,\n",
        "        #\"arvamus\": 67583634,\n",
        "        #\"kultuur\": 67583652,\n",
        "        #\"majandus\": 96669082,\n",
        "        #\"krimi\": 1727246,\n",
        "        #\"tehnoloogia\": 92429137\n",
        "        #\"korvpall\": 24172545,\n",
        "        #\"digi\": 19375415,\n",
        "        #\"teadus\": 19375436,\n",
        "    }\n",
        "\n",
        "    max_articles = 10_000\n",
        "    # Iterate over sections and scrape articles\n",
        "    for section_name, section_id in sections.items():\n",
        "        print(f\"Starting scrape for section: {section_name}\")\n",
        "        articles = collect_articles(section_name, section_id)\n",
        "        if articles:\n",
        "            save_to_csv(section_name, articles)\n",
        "        else:\n",
        "            print(f\"No articles found for section: {section_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get URLs from topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Timeout duration in seconds\n",
        "TIMEOUT = 30\n",
        "\n",
        "# Base API URL for Delfi\n",
        "API_URL = \"https://content.api.delfi.ee/content/v3/graphql\"\n",
        "\n",
        "# Query parameters for topic-based fetching\n",
        "QUERY_PARAMS = {\n",
        "    \"operationName\": \"portal_root_getTags\",\n",
        "    \"extensions\": {\n",
        "        \"persistedQuery\": {\n",
        "            \"version\": 1,\n",
        "            \"sha256Hash\": \"176980ee3361fa76a9757213443e0f8e4f0d62864af7bf97550d248a5f9243a1\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to fetch only the `id` and `url` for a topic\n",
        "def fetch_articles_by_topic(topic_id, limit, offset, retries=3):\n",
        "    variables = {\n",
        "        \"getCount\": True,\n",
        "        \"issueOnly\": \"false\",\n",
        "        \"offset\": offset,\n",
        "        \"limit\": limit,\n",
        "        \"id\": topic_id,\n",
        "        \"authorLanguage\": \"ET\",\n",
        "        \"channelLanguage\": \"ET\",\n",
        "    }\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(\n",
        "                API_URL,\n",
        "                params={\n",
        "                    \"operationName\": QUERY_PARAMS[\"operationName\"],\n",
        "                    \"variables\": json.dumps(variables),\n",
        "                    \"extensions\": json.dumps(QUERY_PARAMS[\"extensions\"]),\n",
        "                },\n",
        "                timeout=TIMEOUT,\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                items = data.get(\"data\", {}).get(\"headlines\", {}).get(\"items\", [])\n",
        "                return [{\"id\": article[\"id\"], \"url\": f\"https://www.delfi.ee/{article['id']}\"} for article in items]\n",
        "            else:\n",
        "                print(f\"Request failed with status code {response.status_code}\")\n",
        "                time.sleep(5)\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching articles for topic {topic_id}: {e}\")\n",
        "            return []\n",
        "    \n",
        "    print(f\"Failed to fetch articles for topic {topic_id} after {retries} attempts.\")\n",
        "    return []\n",
        "\n",
        "# Main function to collect articles by topic with a progress bar\n",
        "def collect_articles_by_topic(topic_name, topic_id, max_articles):\n",
        "    collected_articles = []\n",
        "    limit = 41\n",
        "    offset = 0\n",
        "\n",
        "    with tqdm(total=max_articles, desc=f\"Scraping {topic_name}\") as pbar:\n",
        "        while len(collected_articles) < max_articles and (offset + limit) <= max_articles:\n",
        "            articles = fetch_articles_by_topic(topic_id, limit, offset)\n",
        "            if not articles:\n",
        "                break\n",
        "            collected_articles.extend(articles)\n",
        "            offset += limit\n",
        "            pbar.update(len(articles))\n",
        "\n",
        "    return collected_articles[:max_articles]\n",
        "\n",
        "# Save articles to CSV\n",
        "def save_to_csv(topic_name, articles):\n",
        "    os.makedirs(\"delfi_topics\", exist_ok=True)\n",
        "    file_path = os.path.join(\"delfi_topics\", f\"{topic_name}.csv\")\n",
        "    pd.DataFrame(articles).to_csv(file_path, index=False)\n",
        "    print(f\"Saved {len(articles)} articles to {file_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define topics to scrape (name and topic ID)\n",
        "    topics = {\n",
        "        #\"koroona\": 88761335,\n",
        "        #\"etc\": 123\n",
        "    }\n",
        "\n",
        "    # Set the number of articles to scrape per topic\n",
        "    max_articles = 10000\n",
        "\n",
        "    # Iterate over topics and scrape articles\n",
        "    for topic_name, topic_id in topics.items():\n",
        "        print(f\"Starting scrape for topic: {topic_name}\")\n",
        "        articles = collect_articles_by_topic(topic_name, topic_id, max_articles)\n",
        "        if articles:\n",
        "            save_to_csv(topic_name, articles)\n",
        "        else:\n",
        "            print(f\"No articles found for topic: {topic_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fetch And Update Articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Base API URL for Delfi\n",
        "API_URL = \"https://content.api.delfi.ee/content/v3/graphql\"\n",
        "\n",
        "def clean_html(html_content):\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "    for a in soup.find_all('a'):  # Remove hyperlinks but keep the text\n",
        "        a.unwrap()\n",
        "    return soup.get_text(strip=True)\n",
        "\n",
        "# Fetch article content using the API structure\n",
        "def fetch_article_content(session, article_id):\n",
        "    variables = {\"id\": article_id}\n",
        "    print(article_id)\n",
        "    extensions = {\n",
        "        \"persistedQuery\": {\n",
        "            \"version\": 1,\n",
        "            \"sha256Hash\": \"805a5fb102a1fa46e23693593ca6994fe556d7e087edc3281cf8c940840e1daf\"\n",
        "        }\n",
        "    }\n",
        "    retries = 2\n",
        "    for attempt in range(retries):        \n",
        "        try:\n",
        "            response = session.get(\n",
        "                API_URL,\n",
        "                params={\n",
        "                    \"operationName\": \"portal_root_getArticleBodyByID\",\n",
        "                    \"variables\": json.dumps(variables),\n",
        "                    \"extensions\": json.dumps(extensions)\n",
        "                },\n",
        "                timeout=10\n",
        "            )\n",
        "\n",
        "            # Check if the request was successful\n",
        "            if response.status_code == 200:\n",
        "                response_json = response.json()\n",
        "                article_data = response_json.get(\"data\", {}).get(\"article\", {}).get(\"data\", [])\n",
        "\n",
        "                if not article_data:\n",
        "                    print(f\"No valid article data found for ID {article_id}.\")\n",
        "                    return None\n",
        "\n",
        "                article = article_data[0].get(\"content\", {})  # Use the first element of the list\n",
        "\n",
        "                # Check paywall access\n",
        "                paywall_access = article.get(\"paywall\", {}).get(\"access\", True)\n",
        "                if not paywall_access:\n",
        "                    print(f\"Article ID {article_id} is behind a paywall and cannot be accessed.\")\n",
        "                    return None\n",
        "\n",
        "                # Extract body content if paywall access is granted\n",
        "                lead_content = article.get(\"lead\", {}).get(\"content\", [])\n",
        "                if lead_content:  # Check if the list is not empty\n",
        "                    lead_html = lead_content[0].get(\"html\", \"\")\n",
        "                else:\n",
        "                    lead_html = \"\"  # Default to empty string if no content is found\n",
        "\n",
        "                # Start with cleaned lead content\n",
        "                all_text = clean_html(lead_html)\n",
        "\n",
        "                # Proceed with 'body' content\n",
        "                body_content = article.get(\"body\", {}).get(\"content\", [])\n",
        "                hrefs = []\n",
        "\n",
        "                for fragment in body_content:\n",
        "                    if fragment.get(\"type\") in [\"paragraph\", \"heading\", \"span\"]:\n",
        "                        all_text += \" \" + clean_html(fragment.get(\"html\", \"\"))\n",
        "                    elif fragment.get(\"type\") == \"pullout\":\n",
        "                        all_text += \" \" + clean_html(fragment.get(\"attrs\", {}).get(\"text\", {}).get(\"html\", \"\"))\n",
        "                    elif fragment.get(\"type\") == \"sidebar\":\n",
        "                        title_text = clean_html(fragment.get(\"attrs\", {}).get(\"title\", {}).get(\"text\", \"\")).replace(\"\\n\", \" \")\n",
        "                        all_text += \" \" + title_text\n",
        "\n",
        "                        body_attrs = fragment.get(\"attrs\", {}).get(\"body\", {})\n",
        "                        if 'text' in body_attrs:\n",
        "                            body_text = clean_html(body_attrs.get(\"text\", \"\")).replace(\"\\n\", \" \")\n",
        "                            all_text += \" \" + body_text\n",
        "                        else:\n",
        "                            for content in body_attrs.get(\"content\", []):\n",
        "                                if 'html' in content:\n",
        "                                    body_text = clean_html(content['html']).replace(\"\\n\", \" \")\n",
        "                                    all_text += \" \" + body_text\n",
        "\n",
        "\n",
        "                    # Extract hrefs if present\n",
        "                    if fragment.get(\"html\", \"\"):\n",
        "                        soup = BeautifulSoup(fragment.get(\"html\"), \"html.parser\")\n",
        "                        hrefs.extend([a['href'] for a in soup.find_all('a', href=True)])\n",
        "\n",
        "                return all_text, hrefs\n",
        "\n",
        "            elif response.status_code == 503:\n",
        "                print(f\"Server unavailable, retrying in 1 second... (Attempt {attempt + 1} of {retries})\")\n",
        "                time.sleep(1)\n",
        "                continue\n",
        "        \n",
        "            else:\n",
        "                print(f\"Failed to fetch article body for ID {article_id}. Status code: {response.status_code}\")\n",
        "                return None\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching article body for ID {article_id}: {e}\")\n",
        "            return None\n",
        "    print(\"Failed after maximum retries\")\n",
        "    return None\n",
        "\n",
        "# Update the CSV with the fetched article body\n",
        "def update_csv_with_article_content(session, file_path, delay):\n",
        "    if not file_path:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return\n",
        "\n",
        "    # Read the existing CSV\n",
        "    df = pd.read_csv(file_path)\n",
        "    df[\"content\"] = None\n",
        "    df[\"hrefs\"] = None\n",
        "\n",
        "    # Process each article ID\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Updating {file_path}\"):\n",
        "        if not pd.isnull(row[\"content\"]):\n",
        "            continue\n",
        "\n",
        "        article_id = row[\"id\"]\n",
        "        result = fetch_article_content(session, article_id)\n",
        "        if result:\n",
        "            content, hrefs = result\n",
        "            df.at[index, \"content\"] = content\n",
        "            df.at[index, \"hrefs\"] = hrefs\n",
        "        # Delay between requests\n",
        "        time.sleep(delay)\n",
        "\n",
        "    # Save the updated CSV\n",
        "    df.to_csv(file_path, index=False)\n",
        "    print(f\"Updated file saved to {file_path}\")\n",
        "\n",
        "# Main entry point\n",
        "if __name__ == \"__main__\":\n",
        "    session = requests.Session()\n",
        "    # ADD YOUR BEARER TOKEN HERE\n",
        "    session.headers.update({\n",
        "        \"Authorization\": \"\"\n",
        "    })\n",
        "    \n",
        "    # Define the sections to update\n",
        "    sections = [\"soda\"]\n",
        "    #base_path = \"delfi_sections\"\n",
        "    base_path = \"delfi_topics\"\n",
        "    \n",
        "    for section_name in sections:\n",
        "        csv_file = f\"{base_path}/{section_name}.csv\"\n",
        "        print(f\"Processing {csv_file}...\")\n",
        "        update_csv_with_article_content(session, csv_file, delay=0)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "circularcheck",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
