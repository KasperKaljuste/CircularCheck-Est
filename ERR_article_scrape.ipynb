{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get URLs from Sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Get URLs from sitemap\n",
    "\n",
    "sitemap_url = 'https://www.err.ee/sitemap/sitemap0.xml'\n",
    "\n",
    "response = requests.get(sitemap_url)\n",
    "soup = BeautifulSoup(response.content, 'xml')\n",
    "\n",
    "url_tags = soup.find_all(\"url\")\n",
    "\n",
    "urls = [url.loc.text for url in url_tags]\n",
    "\n",
    "df = pd.DataFrame(urls, columns=[\"URL\"])\n",
    "df.to_excel(\"ERR_URLs.xlsx\", index = False)\n",
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get URLs from Search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fetch_err_urls_paginated(start_date, end_date, limit=50, category=109):\n",
    "    # Base URL\n",
    "    url = \"https://www.err.ee/api/search/getContents/\"\n",
    "    all_urls = []\n",
    "    offset = 0\n",
    "\n",
    "    while True:\n",
    "        # Parameters for the GET request\n",
    "        params = {\n",
    "            \"options\": f'{{\"total\":0,\"limit\":{limit},\"offset\":{offset},\"phrase\":\"\",\"publicStart\":\"{start_date}\",\"publicEnd\":\"{end_date}\",\"timeFromSchedule\":false,\"types\":[],\"category\":{category}}}'\n",
    "        }\n",
    "\n",
    "        # Send GET request\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        # Check the response\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()  # Parse the JSON data\n",
    "\n",
    "            # Extract URLs from the 'contents' list\n",
    "            urls = [item['url'] for item in response_data.get('contents', []) if 'url' in item]\n",
    "\n",
    "\n",
    "            # Break if no more URLs are returned\n",
    "            if not urls:\n",
    "                break\n",
    "\n",
    "            all_urls.extend(urls)\n",
    "            print(f\"Fetched {len(urls)} URLs with offset {offset} ({start_date} to {end_date})\")\n",
    "\n",
    "            # Check if we have fetched all available results\n",
    "            total_found = response_data.get('totalFound', 0)\n",
    "            if len(all_urls) >= total_found:\n",
    "                break\n",
    "\n",
    "            offset += limit  # Increment offset for the next batch\n",
    "        else:\n",
    "            print(\"Request failed with status code:\", response.status_code)\n",
    "            print(\"Response text:\", response.text)  # Print the raw response text for debugging\n",
    "            break\n",
    "\n",
    "    return all_urls\n",
    "\n",
    "def fetch_urls_until_limit(limit_per_request=50, total_limit=100000):\n",
    "    all_urls = []\n",
    "    end_date = datetime.strptime(\"09.02.2025\", \"%d.%m.%Y\")\n",
    "\n",
    "    # Initialize progress bar\n",
    "    with tqdm(total=total_limit, desc=\"Fetching URLs\") as pbar:\n",
    "        while len(all_urls) < total_limit:\n",
    "            start_date = end_date - timedelta(days=30)  # Fetch in 30-day chunks\n",
    "            start_date_str = start_date.strftime(\"%d.%m.%Y\")\n",
    "            end_date_str = end_date.strftime(\"%d.%m.%Y\")\n",
    "\n",
    "            # Fetch all URLs for the range using pagination\n",
    "            urls = fetch_err_urls_paginated(start_date_str, end_date_str, limit=limit_per_request)\n",
    "            all_urls.extend(urls)\n",
    "\n",
    "            print(f\"Fetched {len(urls)} URLs for the period {start_date_str} to {end_date_str}\")\n",
    "\n",
    "            # Update the progress bar and end date\n",
    "            pbar.update(len(urls))\n",
    "            end_date = start_date - timedelta(days=1)  # Move to the previous date range\n",
    "\n",
    "            # Stop if no URLs were fetched in the last iteration\n",
    "            if not urls:\n",
    "                break\n",
    "\n",
    "    print(f\"Collected {len(all_urls)} URLs.\")\n",
    "    return all_urls[:total_limit]  # Ensure we return exactly the required number of URLs\n",
    "\n",
    "# Fetch URLs until we reach the limit\n",
    "urls = fetch_urls_until_limit(total_limit=100_000)\n",
    "df = pd.DataFrame(urls, columns=[\"URL\"])\n",
    "\n",
    "df.to_csv(\"err_100_000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Aggregated Categories of Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def fetch_err_categories_paginated(start_date, end_date, limit=50, category=109):\n",
    "    url = \"https://www.err.ee/api/search/getContents/\"\n",
    "    all_categories = []\n",
    "    offset = 0\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"options\": f'{{\"total\":0,\"limit\":{limit},\"offset\":{offset},\"phrase\":\"\",\"publicStart\":\"{start_date}\",\"publicEnd\":\"{end_date}\",\"timeFromSchedule\":false,\"types\":[],\"category\":{category}}}'\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            contents = response_data.get('contents', [])\n",
    "\n",
    "            if not contents:\n",
    "                break\n",
    "\n",
    "            for item in contents:\n",
    "                name = item.get('primaryCategory', {}).get('name', '')\n",
    "                all_categories.append(name)\n",
    "\n",
    "            total_found = response_data.get('totalFound', 0)\n",
    "            if len(all_categories) >= total_found:\n",
    "                break\n",
    "\n",
    "            offset += limit\n",
    "        else:\n",
    "            print(\"Request failed with status code:\", response.status_code)\n",
    "            break\n",
    "\n",
    "    return all_categories\n",
    "\n",
    "def fetch_categories_until_limit(limit_per_request=50, total_limit=100_000):\n",
    "    all_categories = []\n",
    "    end_date = datetime.strptime(\"09.02.2025\", \"%d.%m.%Y\")\n",
    "\n",
    "    with tqdm(total=total_limit, desc=\"Fetching categories\") as pbar:\n",
    "        while len(all_categories) < total_limit:\n",
    "            start_date = end_date - timedelta(days=30)\n",
    "            start_date_str = start_date.strftime(\"%d.%m.%Y\")\n",
    "            end_date_str = end_date.strftime(\"%d.%m.%Y\")\n",
    "\n",
    "            batch = fetch_err_categories_paginated(start_date_str, end_date_str, limit=limit_per_request)\n",
    "            all_categories.extend(batch)\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "            end_date = start_date - timedelta(days=1)\n",
    "\n",
    "            if not batch:\n",
    "                break\n",
    "\n",
    "    return all_categories[:total_limit]\n",
    "\n",
    "# Run and count\n",
    "categories = fetch_categories_until_limit(total_limit=100_000)\n",
    "counter = Counter(categories)\n",
    "\n",
    "# Print counts\n",
    "for cat, count in counter.most_common():\n",
    "    print(f\"{cat}: {count}\")\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(counter.items(), columns=[\"Category\", \"Count\"])\n",
    "df.to_csv(\"err_100_000_categories.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def fetch_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Title\n",
    "        title = soup.find('h1').text.strip() if soup.find('h1') else 'No title found'\n",
    "\n",
    "        # Content\n",
    "        content_paragraphs = soup.find('div', class_='body').find_all('p') if soup.find('div', class_='body') else []\n",
    "        content = ' '.join([p.text.strip() for p in content_paragraphs])\n",
    "\n",
    "        # References (Only hrefs in the body paragraphs)\n",
    "        references = [a['href'] for p in content_paragraphs for a in p.find_all('a', href=True)]\n",
    "\n",
    "        # Author\n",
    "        author = soup.find('p', class_='editor editor-design')\n",
    "        author = author.find('span', class_='name').text.strip() if author and author.find('span', class_='name') else 'Unknown'\n",
    "\n",
    "        # Column\n",
    "        column = soup.find('div', class_='category').text.strip() if soup.find('div', class_='category') else 'No category'\n",
    "\n",
    "        # Date Published\n",
    "        pubdate = soup.find('time').get('datetime') if soup.find('time') else 'No date'\n",
    "\n",
    "        # Tags\n",
    "        tags = []\n",
    "        tag_section = soup.find('div', class_=\"keywords keywords-design\")\n",
    "        if tag_section:\n",
    "            tags = [tag.text.strip() for tag in tag_section.find_all('a')]\n",
    "\n",
    "        return {\n",
    "            'title': title,\n",
    "            'content': content,\n",
    "            'url': url,\n",
    "            'author': author,\n",
    "            'column': column,\n",
    "            'pubdate': pubdate,\n",
    "            'hrefs': references,\n",
    "            'tags': tags\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def update_news_data(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    results = []\n",
    "\n",
    "    # Using ThreadPoolExecutor to fetch URLs concurrently\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        future_to_url = {executor.submit(fetch_article_content, url): url for url in df['url']}\n",
    "        for future in tqdm(as_completed(future_to_url), total=len(df['url']), desc=\"Processing URLs\"):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "\n",
    "    # Convert list of dicts to DataFrame\n",
    "    news_data = pd.DataFrame(results)\n",
    "\n",
    "    # Save combined data back to CSV with quoting to handle special characters\n",
    "    news_data.to_csv(csv_path, index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    print(f\"Updated data saved to {csv_path}\")\n",
    "\n",
    "csv_path = \"err_articles/err_100_000.csv\"\n",
    "update_news_data(csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "circularcheck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
