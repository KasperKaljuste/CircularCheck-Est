{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Postimees Articles to a Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder = \"postimees_sections\"\n",
    "output_file = \"postimees_combined.csv\"\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(folder, file), usecols=[\"url\", \"hrefs\", \"content\"])\n",
    "        dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Combined {len(dfs)} files into {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Delfi Articles to a Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def combine_and_identify_unique_content(directories_with_files, output_file):\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for directory, files in directories_with_files.items():\n",
    "        for file_name in files:\n",
    "            file_path = Path(directory) / f\"{file_name}.csv\"\n",
    "            if file_path.exists():\n",
    "                df = pd.read_csv(file_path)\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    duplicates = combined_df[combined_df.duplicated(subset='id', keep=False)]\n",
    "    unique_content_duplicates = duplicates.groupby('id').filter(lambda x: not all(x.nunique() == 1))\n",
    "    combined_df.drop_duplicates(subset='id', keep='first').to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"Duplicates with unique content:\")\n",
    "    print(unique_content_duplicates)\n",
    "\n",
    "    return {\n",
    "        \"output_file\": output_file,\n",
    "        \"total_rows\": len(combined_df),\n",
    "        \"unique_rows\": len(combined_df.drop_duplicates(subset='id', keep='first')),\n",
    "        \"duplicates_with_unique_content_count\": len(unique_content_duplicates)\n",
    "    }\n",
    "\n",
    "directories_with_files = {\n",
    "    \"delfi_sections\": [\"arvamus\", \"eesti\", \"jalgpall\", \"kultuur\", \"maailm\", \"korvpall\", \"digi\", \"teadus\"],\n",
    "    \"delfi_topics\": [\"koroona\"]\n",
    "}\n",
    "\n",
    "result = combine_and_identify_unique_content(directories_with_files, \"delfi_combined.csv\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine ERR, Postimees, Delfi Article Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'all_combined.csv'\n",
    "\n",
    "dfs = []\n",
    "postimees_df = pd.read_csv('postimees_combined.csv', usecols=[\"url\", \"hrefs\", \"content\"])\n",
    "delfi_df = pd.read_csv('delfi_combined.csv', usecols=[\"url\", \"hrefs\", \"content\"])\n",
    "err_df = pd.read_csv('err_100_000.csv', usecols=[\"url\", \"hrefs\", \"content\"])\n",
    "\n",
    "all_combined_df = pd.concat([postimees_df, delfi_df, err_df], ignore_index=True)\n",
    "all_combined_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add originalArticle column with value 1 to all\n",
    "df = pd.read_csv(\"all_combined.csv\")\n",
    "\n",
    "df[\"originalArticle\"] = 1\n",
    "df.to_csv(\"all_combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urlextract import URLExtract\n",
    "\n",
    "def filter_hrefs(file_path, output_file):\n",
    "    # Create an instance of URLExtract\n",
    "    extractor = URLExtract()\n",
    "\n",
    "    # Define regex patterns for valid URLs\n",
    "    patterns = [\n",
    "        re.compile(r'err\\.ee/\\d+'),  # Matches 'err.ee' followed by a numeric ID\n",
    "        re.compile(r'postimees\\.ee/\\d+'),  # Matches 'postimees.ee' followed by a numeric ID\n",
    "        re.compile(r'delfi\\.ee/artikkel/\\d+')  # Matches 'delfi.ee/artikkel/' followed by a numeric ID\n",
    "    ]\n",
    "\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path, dtype={'hrefs': str})\n",
    "    \n",
    "    # Ensure only non-null and non-empty hrefs are processed\n",
    "    df = df[df['hrefs'].notna() & (df['hrefs'] != '[]')]\n",
    "\n",
    "    # Extract URLs from each entry in the 'hrefs' column and filter them\n",
    "    def extract_and_filter_urls(text):\n",
    "        urls = extractor.find_urls(text)\n",
    "        # Filter URLs to include only those matching the specific regex patterns\n",
    "        filtered_urls = []\n",
    "        for url in urls:\n",
    "            if any(pattern.search(url) for pattern in patterns):\n",
    "                filtered_urls.append(url)\n",
    "        return filtered_urls\n",
    "\n",
    "    df['extracted_hrefs'] = df['hrefs'].apply(extract_and_filter_urls)\n",
    "\n",
    "    # Filter out rows where no URLs were extracted\n",
    "    df = df[df['extracted_hrefs'].map(len) > 0]\n",
    "\n",
    "    # Select only the 'url' and 'extracted_hrefs' columns\n",
    "    df = df[['url', 'extracted_hrefs']]\n",
    "    \n",
    "    # Save the processed DataFrame to a CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "filtered_df = filter_hrefs(\"all_combined.csv\", \"all_filtered.csv\")\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct Delfi URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to modify Delfi URLs\n",
    "def modify_delfi_url(url):\n",
    "    if \"https://www.delfi.ee/\" in url and \"/artikkel/\" not in url:\n",
    "        parts = url.split(\"/\")\n",
    "        new_url = f\"https://www.delfi.ee/artikkel/{parts[-1]}\"\n",
    "        return new_url\n",
    "    return url\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('all_filtered.csv')\n",
    "\n",
    "# Modify the 'url' column\n",
    "df['url'] = df['url'].apply(modify_delfi_url)\n",
    "\n",
    "# Save the changes back to the CSV file\n",
    "df.to_csv('all_filtered_modified_delfi.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circularcheck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
